{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81ca92c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== üß© Training Dual Expert v3.0 ‚Äî InfoNCE Edition ===\n",
      "‚úÖ Loaded 20622 Advice samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:   0%|          | 0/644 [00:00<?, ?it/s]/tmp/ipykernel_146102/3902516364.py:197: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  has_ctx = torch.tensor(has_ctx, dtype=torch.float32, device=DEVICE)\n",
      "Epoch 1/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 644/644 [01:05<00:00,  9.89it/s, anchor=0.9520, ctx=2.1182, hasctx=0.5373, loss=5.4571]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 1 | Loss=5.4486 | Anchor=0.9505 | Context=2.1149 | HasCtx=0.5365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 644/644 [01:02<00:00, 10.23it/s, anchor=0.0984, ctx=1.7833, hasctx=0.3746, loss=3.8522]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 2 | Loss=3.8522 | Anchor=0.0984 | Context=1.7833 | HasCtx=0.3746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 644/644 [01:03<00:00, 10.14it/s, anchor=0.0782, ctx=1.7693, hasctx=0.2975, loss=3.7656]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 3 | Loss=3.7656 | Anchor=0.0782 | Context=1.7693 | HasCtx=0.2975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 644/644 [01:03<00:00, 10.21it/s, anchor=0.0673, ctx=1.7606, hasctx=0.2345, loss=3.7056]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 4 | Loss=3.7056 | Anchor=0.0673 | Context=1.7606 | HasCtx=0.2345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 644/644 [01:01<00:00, 10.42it/s, anchor=0.0644, ctx=1.7593, hasctx=0.1887, loss=3.6774]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 5 | Loss=3.6774 | Anchor=0.0644 | Context=1.7593 | HasCtx=0.1887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 644/644 [01:03<00:00, 10.19it/s, anchor=0.0609, ctx=1.7587, hasctx=0.1583, loss=3.6575]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 6 | Loss=3.6575 | Anchor=0.0609 | Context=1.7587 | HasCtx=0.1583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 644/644 [01:04<00:00, 10.05it/s, anchor=0.0615, ctx=1.7550, hasctx=0.1381, loss=3.6406]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 7 | Loss=3.6406 | Anchor=0.0615 | Context=1.7550 | HasCtx=0.1381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 644/644 [01:02<00:00, 10.28it/s, anchor=0.0573, ctx=1.7603, hasctx=0.1239, loss=3.6397]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 8 | Loss=3.6397 | Anchor=0.0573 | Context=1.7603 | HasCtx=0.1239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 644/644 [01:02<00:00, 10.29it/s, anchor=0.0591, ctx=1.7572, hasctx=0.1135, loss=3.6302]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 9 | Loss=3.6246 | Anchor=0.0590 | Context=1.7544 | HasCtx=0.1133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 644/644 [01:02<00:00, 10.29it/s, anchor=0.0529, ctx=1.7545, hasctx=0.1049, loss=3.6143]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 10 | Loss=3.6143 | Anchor=0.0529 | Context=1.7545 | HasCtx=0.1049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 644/644 [01:03<00:00, 10.22it/s, anchor=0.0563, ctx=1.7493, hasctx=0.0979, loss=3.6038]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 11 | Loss=3.6038 | Anchor=0.0563 | Context=1.7493 | HasCtx=0.0979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 644/644 [01:04<00:00, 10.02it/s, anchor=0.0514, ctx=1.7474, hasctx=0.0918, loss=3.5921]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 12 | Loss=3.5921 | Anchor=0.0514 | Context=1.7474 | HasCtx=0.0918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 644/644 [01:04<00:00,  9.95it/s, anchor=0.0544, ctx=1.7580, hasctx=0.0868, loss=3.6138]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 13 | Loss=3.6138 | Anchor=0.0544 | Context=1.7580 | HasCtx=0.0868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 644/644 [01:05<00:00,  9.83it/s, anchor=0.0507, ctx=1.7470, hasctx=0.0821, loss=3.5858]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 14 | Loss=3.5858 | Anchor=0.0507 | Context=1.7470 | HasCtx=0.0821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 644/644 [01:01<00:00, 10.45it/s, anchor=0.0488, ctx=1.7528, hasctx=0.0783, loss=3.5937]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 15 | Loss=3.5937 | Anchor=0.0488 | Context=1.7528 | HasCtx=0.0783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 644/644 [01:03<00:00, 10.11it/s, anchor=0.0497, ctx=1.7502, hasctx=0.0747, loss=3.5874]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 16 | Loss=3.5818 | Anchor=0.0496 | Context=1.7474 | HasCtx=0.0746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 644/644 [01:02<00:00, 10.28it/s, anchor=0.0501, ctx=1.7521, hasctx=0.0713, loss=3.5899]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 17 | Loss=3.5899 | Anchor=0.0501 | Context=1.7521 | HasCtx=0.0713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 644/644 [01:02<00:00, 10.27it/s, anchor=0.0475, ctx=1.7521, hasctx=0.0682, loss=3.5858]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 18 | Loss=3.5858 | Anchor=0.0475 | Context=1.7521 | HasCtx=0.0682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 644/644 [01:02<00:00, 10.28it/s, anchor=0.0459, ctx=1.7495, hasctx=0.0655, loss=3.5776]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 19 | Loss=3.5776 | Anchor=0.0459 | Context=1.7495 | HasCtx=0.0655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 644/644 [01:03<00:00, 10.21it/s, anchor=0.0526, ctx=1.7556, hasctx=0.0628, loss=3.5953]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 20 | Loss=3.5897 | Anchor=0.0525 | Context=1.7529 | HasCtx=0.0627\n",
      "\n",
      "üíæ Saved model & adapter to: models_anchor/dual_expert_Advice_v3_0\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üß† Dual Neural Expert v3.0 ‚Äî InfoNCE + True ST Embedding\n",
    "#  - M·ª•c ti√™u:\n",
    "#    ‚Ä¢ Anchor@1 cao h∆°n nh·ªù in-batch hard negatives (InfoNCE)\n",
    "#    ‚Ä¢ Context@1 ·ªïn ƒë·ªãnh, kh√¥ng l·ªách so v·ªõi encoder g·ªëc\n",
    "#    ‚Ä¢ Kh√¥ng ƒë·ª•ng internal .auto_model, d√πng SentenceTransformer ƒë√∫ng chu·∫©n\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# ‚öôÔ∏è CONFIGURATION\n",
    "# ------------------------------------------------------------\n",
    "DATA_FILE = \"Advice.csv\"                      # synthetic Advice dataset\n",
    "MODEL_NAME = \"google/embeddinggemma-300m\"     # base encoder\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "LR = 2e-5\n",
    "SEED = 42\n",
    "\n",
    "# InfoNCE temperature\n",
    "TEMP_ANCHOR = 0.07\n",
    "TEMP_CONTEXT = 0.07\n",
    "\n",
    "# Loss weights\n",
    "LAMBDA_ANCHOR  = 1.0\n",
    "LAMBDA_CONTEXT = 2.0   # boost context\n",
    "LAMBDA_HASCTX  = 0.5\n",
    "\n",
    "SAVE_PATH = \"models_anchor/dual_expert_Advice_v3_0\"\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# üß© Dataset\n",
    "#   Expect columns: question, expert_label, anchor, has_context, context_name\n",
    "# ------------------------------------------------------------\n",
    "class DualAnchorDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.data = df.to_dict(\"records\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data[idx]\n",
    "\n",
    "        q = str(row[\"question\"])\n",
    "        anchor = str(row[\"anchor\"])\n",
    "\n",
    "        has_context = int(row.get(\"has_context\", 0))\n",
    "\n",
    "        if has_context == 1 and pd.notna(row.get(\"context_name\", \"\")):\n",
    "            context = str(row[\"context_name\"])\n",
    "        else:\n",
    "            context = \"\"\n",
    "\n",
    "        return q, anchor, context, has_context\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# üß† Dual Expert Adapter\n",
    "#   - Nh·∫≠n embedding t·ª´ SentenceTransformer\n",
    "#   - Tr·∫£ ra anchor_emb, context_emb, has_context_logit\n",
    "# ------------------------------------------------------------\n",
    "class DualExpertAdapter(nn.Module):\n",
    "    def __init__(self, input_dim=768, hidden_dim=256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.anchor_adapter = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "\n",
    "        self.context_adapter = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "\n",
    "        self.has_context_head = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, D) from SentenceTransformer\n",
    "        anchor_emb = F.normalize(self.anchor_adapter(x), p=2, dim=1)\n",
    "        context_emb = F.normalize(self.context_adapter(x), p=2, dim=1)\n",
    "        has_ctx_logit = self.has_context_head(x).squeeze(-1)\n",
    "        return anchor_emb, context_emb, has_ctx_logit\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# üîß Helper: encode text with SentenceTransformer\n",
    "#   - ƒê·∫£m b·∫£o d√πng pooling chu·∫©n c·ªßa SentenceTransformer\n",
    "# ------------------------------------------------------------\n",
    "def encode_texts(encoder: SentenceTransformer, texts, batch_size=64):\n",
    "    # texts: list[str]\n",
    "    # Tr·∫£ v·ªÅ: Tensor (N, D) tr√™n DEVICE\n",
    "    with torch.no_grad():\n",
    "        emb = encoder.encode(\n",
    "            texts,\n",
    "            batch_size=batch_size,\n",
    "            convert_to_tensor=True,\n",
    "            device=DEVICE,\n",
    "            show_progress_bar=False\n",
    "        )\n",
    "    return emb\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# üöÄ Training loop ‚Äî Dual Expert v3.0\n",
    "# ------------------------------------------------------------\n",
    "def train_dual_expert_v3():\n",
    "    print(\"\\n=== üß© Training Dual Expert v3.0 ‚Äî InfoNCE Edition ===\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Load & clean data\n",
    "    # -----------------------------\n",
    "    df = pd.read_csv(DATA_FILE)\n",
    "    df = df[df[\"expert_label\"] == \"Advice\"].dropna(subset=[\"question\", \"anchor\"])\n",
    "\n",
    "    if \"context_name\" not in df.columns:\n",
    "        df[\"context_name\"] = \"\"\n",
    "    if \"has_context\" not in df.columns:\n",
    "        df[\"has_context\"] = 0\n",
    "\n",
    "    df[\"has_context\"] = df[\"has_context\"].fillna(0).astype(int)\n",
    "\n",
    "    # ƒê·∫£m b·∫£o: n·∫øu has_context = 0 th√¨ context_name = \"\"\n",
    "    df[\"context_name\"] = df.apply(\n",
    "        lambda x: str(x[\"context_name\"]) if x[\"has_context\"] == 1 and pd.notna(x[\"context_name\"]) else \"\",\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    print(f\"‚úÖ Loaded {len(df)} Advice samples\")\n",
    "\n",
    "    dataset = DualAnchorDataset(df)\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Base encoder (frozen)\n",
    "    # -----------------------------\n",
    "    base_encoder = SentenceTransformer(MODEL_NAME)\n",
    "    base_encoder.to(DEVICE)\n",
    "    base_encoder.eval()\n",
    "    for p in base_encoder.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # L·∫•y dim t·ª´ SentenceTransformer (an to√†n)\n",
    "    try:\n",
    "        input_dim = base_encoder.get_sentence_embedding_dimension()\n",
    "    except Exception:\n",
    "        # fallback n·∫øu model kh√¥ng c√≥ h√†m n√†y\n",
    "        test_emb = encode_texts(base_encoder, [\"test\"])\n",
    "        input_dim = test_emb.size(1)\n",
    "\n",
    "    adapter = DualExpertAdapter(input_dim=input_dim, hidden_dim=256).to(DEVICE)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(adapter.parameters(), lr=LR)\n",
    "    bce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    use_amp = DEVICE == \"cuda\"\n",
    "    scaler = torch.amp.GradScaler(\"cuda\") if use_amp else None\n",
    "\n",
    "    # -----------------------------\n",
    "    # Training epochs\n",
    "    # -----------------------------\n",
    "    for epoch in range(EPOCHS):\n",
    "        adapter.train()\n",
    "        total_loss = 0.0\n",
    "        total_anchor_loss = 0.0\n",
    "        total_ctx_loss = 0.0\n",
    "        total_hasctx_loss = 0.0\n",
    "\n",
    "        pbar = tqdm(loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "\n",
    "        for batch in pbar:\n",
    "            q_texts, a_texts, c_texts, has_ctx = batch\n",
    "            has_ctx = torch.tensor(has_ctx, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "            if use_amp:\n",
    "                ctx_mgr = torch.amp.autocast(\"cuda\")\n",
    "            else:\n",
    "                # no-op context manager\n",
    "                from contextlib import nullcontext\n",
    "                ctx_mgr = nullcontext()\n",
    "\n",
    "            with ctx_mgr:\n",
    "                # 1) Encode texts -> embeddings (SentenceTransformer pooling chu·∫©n)\n",
    "                q_emb = encode_texts(base_encoder, list(q_texts))\n",
    "                a_emb = encode_texts(base_encoder, list(a_texts))\n",
    "\n",
    "                # Context embeddings: encode c·∫£ batch (k·ªÉ c·∫£ ch·ªó r·ªóng, s·∫Ω mask sau)\n",
    "                c_emb = encode_texts(base_encoder, list(c_texts))\n",
    "\n",
    "                # 2) Adapter forward\n",
    "                q_anchor, q_ctx, has_ctx_logit = adapter(q_emb)\n",
    "                a_anchor, _, _ = adapter(a_emb)\n",
    "                _, c_ctx, _ = adapter(c_emb)\n",
    "\n",
    "                # ========== Anchor InfoNCE Loss ==========\n",
    "                # sim(q_i, a_j) cho to√†n batch\n",
    "                sim_anchor = torch.matmul(q_anchor, a_anchor.T) / TEMP_ANCHOR   # (B, B)\n",
    "                target = torch.arange(sim_anchor.size(0), device=DEVICE)       # m·ªói q kh·ªõp anchor c√πng index\n",
    "                loss_anchor = F.cross_entropy(sim_anchor, target)\n",
    "\n",
    "                # ========== Context InfoNCE Loss ==========\n",
    "                mask_ctx = has_ctx > 0.5\n",
    "                if mask_ctx.sum() > 1:\n",
    "                    q_ctx_pos = q_ctx[mask_ctx]\n",
    "                    c_ctx_pos = c_ctx[mask_ctx]\n",
    "\n",
    "                    # in-batch similarity\n",
    "                    sim_ctx = torch.matmul(q_ctx_pos, c_ctx_pos.T) / TEMP_CONTEXT\n",
    "                    target_ctx = torch.arange(sim_ctx.size(0), device=DEVICE)\n",
    "                    loss_ctx = F.cross_entropy(sim_ctx, target_ctx)\n",
    "                else:\n",
    "                    loss_ctx = torch.tensor(0.0, device=DEVICE)\n",
    "\n",
    "                # ========== HasContext BCE ==========\n",
    "                loss_hasctx = bce_loss(has_ctx_logit, has_ctx)\n",
    "\n",
    "                # ========== Total loss ==========\n",
    "                loss = (\n",
    "                    LAMBDA_ANCHOR  * loss_anchor +\n",
    "                    LAMBDA_CONTEXT * loss_ctx +\n",
    "                    LAMBDA_HASCTX  * loss_hasctx\n",
    "                )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            if use_amp:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_anchor_loss += loss_anchor.item()\n",
    "            total_ctx_loss += loss_ctx.item()\n",
    "            total_hasctx_loss += loss_hasctx.item()\n",
    "\n",
    "            step = pbar.n + 1\n",
    "            pbar.set_postfix(\n",
    "                loss=f\"{total_loss/step:.4f}\",\n",
    "                anchor=f\"{total_anchor_loss/step:.4f}\",\n",
    "                ctx=f\"{total_ctx_loss/step:.4f}\",\n",
    "                hasctx=f\"{total_hasctx_loss/step:.4f}\"\n",
    "            )\n",
    "\n",
    "        n_steps = len(loader)\n",
    "        print(\n",
    "            f\"‚úÖ Epoch {epoch+1} | \"\n",
    "            f\"Loss={total_loss/n_steps:.4f} | \"\n",
    "            f\"Anchor={total_anchor_loss/n_steps:.4f} | \"\n",
    "            f\"Context={total_ctx_loss/n_steps:.4f} | \"\n",
    "            f\"HasCtx={total_hasctx_loss/n_steps:.4f}\"\n",
    "        )\n",
    "\n",
    "    # -----------------------------\n",
    "    # Save adapter + encoder\n",
    "    # -----------------------------\n",
    "    torch.save(adapter.state_dict(), os.path.join(SAVE_PATH, \"dual_adapter_v3.pt\"))\n",
    "    base_encoder.save(SAVE_PATH)\n",
    "\n",
    "    print(f\"\\nüíæ Saved model & adapter to: {SAVE_PATH}\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    train_dual_expert_v3()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c5165b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading SentenceTransformer encoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from 'models_anchor/dual_expert_Advice_v3_0' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìê Embedding dimension = 768\n",
      "üîÑ Loading DualExpertAdapter...\n",
      "üìå Loaded 474 anchors\n",
      "üìå Loaded 5 contexts\n",
      "\n",
      "‚ö° Encoding anchors...\n",
      "‚ö° Encoding contexts...\n",
      "\n",
      "üí¨ Ready! Nh·∫≠p c√¢u h·ªèi ƒë·ªÉ test (g√µ 'exit' ƒë·ªÉ tho√°t).\n",
      "\n",
      "==========================\n",
      "üîé QUESTION: T√¥i b·ªã HIV trong thai k·ª≥ th√¨ n√™n l√†m g√¨?\n",
      "==========================\n",
      "\n",
      "üéØ TOP ANCHORS:\n",
      "  ‚Ä¢ HIV                            ‚Üí 0.7663\n",
      "  ‚Ä¢ AIDS                           ‚Üí 0.5159\n",
      "  ‚Ä¢ ƒêau khi quan h·ªá t√¨nh d·ª•c       ‚Üí 0.3026\n",
      "  ‚Ä¢ Virus Zika                     ‚Üí 0.2919\n",
      "  ‚Ä¢ Vi√™m gan si√™u vi B             ‚Üí 0.2770\n",
      "\n",
      "üîÆ Has-Context Probability: 0.9866\n",
      "\n",
      "üåç TOP CONTEXTS:\n",
      "  ‚Ä¢ Thai k·ª≥                        ‚Üí 0.7867\n",
      "  ‚Ä¢ Tr∆∞·ªõc thai k·ª≥                  ‚Üí -0.0606\n",
      "  ‚Ä¢ Cu·ªëi thai k·ª≥                   ‚Üí -0.0759\n",
      "  ‚Ä¢ Qu√° tr√¨nh sinh n·ªü              ‚Üí -0.3710\n",
      "  ‚Ä¢ Chuy·ªÉn d·∫°                      ‚Üí -0.3936\n",
      "\n",
      "==========================\n",
      "üîé QUESTION: T√¥i b·ªã hiv trong thai k·ª≥ th√¨ n√™n l√†m g√¨\n",
      "==========================\n",
      "\n",
      "üéØ TOP ANCHORS:\n",
      "  ‚Ä¢ HIV                            ‚Üí 0.7456\n",
      "  ‚Ä¢ AIDS                           ‚Üí 0.4521\n",
      "  ‚Ä¢ Vi√™m gan si√™u vi B             ‚Üí 0.2904\n",
      "  ‚Ä¢ Ung th∆∞ √¢m h·ªô                  ‚Üí 0.2836\n",
      "  ‚Ä¢ ƒêau khi quan h·ªá t√¨nh d·ª•c       ‚Üí 0.2770\n",
      "\n",
      "üîÆ Has-Context Probability: 0.9918\n",
      "\n",
      "üåç TOP CONTEXTS:\n",
      "  ‚Ä¢ Thai k·ª≥                        ‚Üí 0.7420\n",
      "  ‚Ä¢ Cu·ªëi thai k·ª≥                   ‚Üí 0.0457\n",
      "  ‚Ä¢ Chuy·ªÉn d·∫°                      ‚Üí -0.1840\n",
      "  ‚Ä¢ Tr∆∞·ªõc thai k·ª≥                  ‚Üí -0.2201\n",
      "  ‚Ä¢ Qu√° tr√¨nh sinh n·ªü              ‚Üí -0.4755\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 176\u001b[39m\n\u001b[32m    173\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müí¨ Ready! Nh·∫≠p c√¢u h·ªèi ƒë·ªÉ test (g√µ \u001b[39m\u001b[33m'\u001b[39m\u001b[33mexit\u001b[39m\u001b[33m'\u001b[39m\u001b[33m ƒë·ªÉ tho√°t).\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     q = \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m‚ùì C√¢u h·ªèi: \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.strip()\n\u001b[32m    177\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m q.lower() == \u001b[33m\"\u001b[39m\u001b[33mexit\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    178\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müëã Bye!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py:1282\u001b[39m, in \u001b[36mKernel.raw_input\u001b[39m\u001b[34m(self, prompt)\u001b[39m\n\u001b[32m   1280\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1281\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[32m-> \u001b[39m\u001b[32m1282\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1283\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1284\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1285\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1286\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1287\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py:1325\u001b[39m, in \u001b[36mKernel._input_request\u001b[39m\u001b[34m(self, prompt, ident, parent, password)\u001b[39m\n\u001b[32m   1322\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1323\u001b[39m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[32m   1324\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mInterrupted by user\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1325\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1326\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1327\u001b[39m     \u001b[38;5;28mself\u001b[39m.log.warning(\u001b[33m\"\u001b[39m\u001b[33mInvalid Message:\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üîç Dual Expert v3.0 ‚Äî TEST FILE (FIXED VERSION)\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# üß† Dual Expert Adapter (same as training)\n",
    "# ============================================================\n",
    "\n",
    "class DualExpertAdapter(torch.nn.Module):\n",
    "    def __init__(self, input_dim=768, hidden_dim=256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.anchor_adapter = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "\n",
    "        self.context_adapter = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "\n",
    "        self.has_context_head = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        anchor_emb = F.normalize(self.anchor_adapter(x), p=2, dim=1)\n",
    "        context_emb = F.normalize(self.context_adapter(x), p=2, dim=1)\n",
    "        has_ctx_logit = self.has_context_head(x).squeeze(-1)\n",
    "        return anchor_emb, context_emb, has_ctx_logit\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ‚öô CONFIG\n",
    "# ============================================================\n",
    "\n",
    "MODEL_DIR = \"models_anchor/dual_expert_Advice_v3_0\"   # folder model b·∫°n l∆∞u\n",
    "ADAPTER_PATH = f\"{MODEL_DIR}/dual_adapter_v3.pt\"\n",
    "DATA_FILE = \"Advice.csv\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "TOP_K = 5\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# üöÄ LOAD MODEL\n",
    "# ============================================================\n",
    "\n",
    "print(\"üîÑ Loading SentenceTransformer encoder...\")\n",
    "encoder = SentenceTransformer(MODEL_DIR).to(DEVICE)\n",
    "encoder.eval()\n",
    "\n",
    "try:\n",
    "    emb_dim = encoder.get_sentence_embedding_dimension()\n",
    "except:\n",
    "    emb_dim = encoder.encode([\"hi\"], convert_to_tensor=True).shape[1]\n",
    "\n",
    "print(f\"üìê Embedding dimension = {emb_dim}\")\n",
    "\n",
    "\n",
    "print(\"üîÑ Loading DualExpertAdapter...\")\n",
    "adapter = DualExpertAdapter(input_dim=emb_dim)\n",
    "adapter.load_state_dict(torch.load(ADAPTER_PATH, map_location=DEVICE))\n",
    "adapter = adapter.to(DEVICE)\n",
    "adapter.eval()\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# üì• LOAD ANCHORS + CONTEXTS\n",
    "# ============================================================\n",
    "\n",
    "df = pd.read_csv(DATA_FILE)\n",
    "df = df[df[\"expert_label\"] == \"Advice\"]\n",
    "\n",
    "anchors = sorted(df[\"anchor\"].unique())\n",
    "contexts = sorted([c for c in df[\"context_name\"].unique() if isinstance(c, str) and c.strip() != \"\"])\n",
    "\n",
    "print(f\"üìå Loaded {len(anchors)} anchors\")\n",
    "print(f\"üìå Loaded {len(contexts)} contexts\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ‚ö° PRE-ENCODE ANCHORS + CONTEXTS (FIXED)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n‚ö° Encoding anchors...\")\n",
    "anchor_emb = encoder.encode(anchors, convert_to_tensor=True, device=DEVICE)\n",
    "anchor_emb = anchor_emb.detach().clone()\n",
    "anchor_emb = adapter.anchor_adapter(anchor_emb)\n",
    "anchor_emb = F.normalize(anchor_emb, p=2, dim=1)\n",
    "\n",
    "\n",
    "print(\"‚ö° Encoding contexts...\")\n",
    "if len(contexts) > 0:\n",
    "    context_emb = encoder.encode(contexts, convert_to_tensor=True, device=DEVICE)\n",
    "    context_emb = context_emb.detach().clone()\n",
    "    context_emb = adapter.context_adapter(context_emb)\n",
    "    context_emb = F.normalize(context_emb, p=2, dim=1)\n",
    "else:\n",
    "    context_emb = None\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# üîç PREDICT FUNCTION\n",
    "# ============================================================\n",
    "\n",
    "def predict(question, top_k=TOP_K):\n",
    "    print(\"\\n==========================\")\n",
    "    print(f\"üîé QUESTION: {question}\")\n",
    "    print(\"==========================\")\n",
    "\n",
    "    # Encode question\n",
    "    q_emb = encoder.encode([question], convert_to_tensor=True, device=DEVICE)\n",
    "    q_emb = q_emb.detach().clone()\n",
    "\n",
    "    q_anchor, q_ctx, q_hasctx = adapter(q_emb)\n",
    "\n",
    "    # ---- ANCHOR SIM ----\n",
    "    sim_anchor = (q_anchor @ anchor_emb.T)[0]\n",
    "    top_anchor_idx = torch.topk(sim_anchor, top_k).indices.tolist()\n",
    "    top_anchors = [(anchors[i], float(sim_anchor[i])) for i in top_anchor_idx]\n",
    "\n",
    "    # ---- HAS-CONTEXT ----\n",
    "    has_ctx_prob = torch.sigmoid(q_hasctx).item()\n",
    "\n",
    "    # ---- CONTEXT SIM ----\n",
    "    if context_emb is not None:\n",
    "        sim_ctx = (q_ctx @ context_emb.T)[0]\n",
    "        top_ctx_idx = torch.topk(sim_ctx, top_k).indices.tolist()\n",
    "        top_contexts = [(contexts[i], float(sim_ctx[i])) for i in top_ctx_idx]\n",
    "    else:\n",
    "        top_contexts = []\n",
    "\n",
    "    # PRINT\n",
    "    print(\"\\nüéØ TOP ANCHORS:\")\n",
    "    for name, score in top_anchors:\n",
    "        print(f\"  ‚Ä¢ {name:<30} ‚Üí {score:.4f}\")\n",
    "\n",
    "    print(f\"\\nüîÆ Has-Context Probability: {has_ctx_prob:.4f}\")\n",
    "\n",
    "    print(\"\\nüåç TOP CONTEXTS:\")\n",
    "    for name, score in top_contexts:\n",
    "        print(f\"  ‚Ä¢ {name:<30} ‚Üí {score:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"anchors\": top_anchors,\n",
    "        \"contexts\": top_contexts,\n",
    "        \"has_context\": has_ctx_prob\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# üß™ INTERACTIVE CHAT MODE (ENTER LI√äN T·ª§C)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nüí¨ Ready! Nh·∫≠p c√¢u h·ªèi ƒë·ªÉ test (g√µ 'exit' ƒë·ªÉ tho√°t).\")\n",
    "\n",
    "while True:\n",
    "    q = input(\"\\n‚ùì C√¢u h·ªèi: \").strip()\n",
    "    if q.lower() == \"exit\":\n",
    "        print(\"üëã Bye!\")\n",
    "        break\n",
    "    if q == \"\":\n",
    "        continue   # enter r·ªóng th√¨ b·ªè qua\n",
    "    \n",
    "    predict(q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dcebce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading SentenceTransformer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from 'models_anchor/dual_expert_Advice_v3_0' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìê Embedding Dimension = 768\n",
      "üîÑ Loading Adapter...\n",
      "üìå Found 137 anchors\n",
      "üìå Found 5 contexts\n",
      "‚ö° Encoding anchors...\n",
      "‚ö° Encoding contexts...\n",
      "\n",
      "üöÄ Starting Evaluation...\n",
      "\n",
      "\n",
      "===============================\n",
      "üìä FINAL EVALUATION RESULTS\n",
      "===============================\n",
      "\n",
      "üéØ Anchor Accuracy:\n",
      "  Top-1: 0.9948\n",
      "  Top-3: 1.0000\n",
      "  Top-5: 1.0000\n",
      "\n",
      "üåç Context Accuracy (only samples with context):\n",
      "  Top-1: 1.0000\n",
      "  Top-3: 1.0000\n",
      "  Top-5: 1.0000\n",
      "\n",
      "üîÆ Has-Context F1 Score:\n",
      "  F1: 0.9967\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üìä Dual Expert v3.0 ‚Äî EVALUATION SCRIPT\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# üß† Load Adapter (same as training)\n",
    "# ============================================================\n",
    "\n",
    "class DualExpertAdapter(torch.nn.Module):\n",
    "    def __init__(self, input_dim=768, hidden_dim=256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.anchor_adapter = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "\n",
    "        self.context_adapter = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "\n",
    "        self.has_context_head = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        anchor_emb = F.normalize(self.anchor_adapter(x), p=2, dim=1)\n",
    "        context_emb = F.normalize(self.context_adapter(x), p=2, dim=1)\n",
    "        has_ctx_logit = self.has_context_head(x).squeeze(-1)\n",
    "        return anchor_emb, context_emb, has_ctx_logit\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ‚öô CONFIG\n",
    "# ============================================================\n",
    "\n",
    "MODEL_DIR = \"models_anchor/dual_expert_Advice_v3_0\"\n",
    "ADAPTER_PATH = f\"{MODEL_DIR}/dual_adapter_v3.pt\"\n",
    "DATA_FILE = \"Advice_test.csv\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "TOP_K = [1, 3, 5]\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# üöÄ Load Encoder + Adapter\n",
    "# ============================================================\n",
    "\n",
    "print(\"üîÑ Loading SentenceTransformer...\")\n",
    "encoder = SentenceTransformer(MODEL_DIR).to(DEVICE)\n",
    "encoder.eval()\n",
    "\n",
    "try:\n",
    "    emb_dim = encoder.get_sentence_embedding_dimension()\n",
    "except:\n",
    "    emb_dim = encoder.encode([\"hi\"], convert_to_tensor=True).shape[1]\n",
    "\n",
    "print(f\"üìê Embedding Dimension = {emb_dim}\")\n",
    "\n",
    "print(\"üîÑ Loading Adapter...\")\n",
    "adapter = DualExpertAdapter(input_dim=emb_dim)\n",
    "adapter.load_state_dict(torch.load(ADAPTER_PATH, map_location=DEVICE))\n",
    "adapter = adapter.to(DEVICE)\n",
    "adapter.eval()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# üì• Load Dataset Anchors + Contexts\n",
    "# ============================================================\n",
    "\n",
    "df = pd.read_csv(DATA_FILE)\n",
    "df = df[df[\"expert_label\"] == \"Advice\"]\n",
    "\n",
    "anchors = sorted(df[\"anchor\"].unique())\n",
    "contexts = sorted([c for c in df[\"context_name\"].unique() if isinstance(c, str) and c.strip() != \"\"])\n",
    "\n",
    "print(f\"üìå Found {len(anchors)} anchors\")\n",
    "print(f\"üìå Found {len(contexts)} contexts\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ‚ö° Pre-encode Anchors + Contexts\n",
    "# ============================================================\n",
    "\n",
    "print(\"‚ö° Encoding anchors...\")\n",
    "anchor_emb = encoder.encode(anchors, convert_to_tensor=True, device=DEVICE)\n",
    "anchor_emb = anchor_emb.detach().clone()\n",
    "anchor_emb = adapter.anchor_adapter(anchor_emb)\n",
    "anchor_emb = F.normalize(anchor_emb, p=2, dim=1)\n",
    "\n",
    "print(\"‚ö° Encoding contexts...\")\n",
    "if len(contexts) > 0:\n",
    "    context_emb = encoder.encode(contexts, convert_to_tensor=True, device=DEVICE)\n",
    "    context_emb = context_emb.detach().clone()\n",
    "    context_emb = adapter.context_adapter(context_emb)\n",
    "    context_emb = F.normalize(context_emb, p=2, dim=1)\n",
    "else:\n",
    "    context_emb = None\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# üìä Evaluation Functions\n",
    "# ============================================================\n",
    "\n",
    "def topk_check(name_list, sim_scores, gold_name, k):\n",
    "    top_k_idx = torch.topk(sim_scores, k).indices.tolist()\n",
    "    top_k_names = [name_list[i] for i in top_k_idx]\n",
    "    return gold_name in top_k_names\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# üìä Main Evaluation\n",
    "# ============================================================\n",
    "\n",
    "def evaluate():\n",
    "    print(\"\\nüöÄ Starting Evaluation...\\n\")\n",
    "\n",
    "    anchor_correct = {1: 0, 3: 0, 5: 0}\n",
    "    context_correct = {1: 0, 3: 0, 5: 0}\n",
    "    hasctx_true = []\n",
    "    hasctx_pred = []\n",
    "\n",
    "    total = len(df)\n",
    "    ctx_total = len(df[df[\"has_context\"] == 1])\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        q = row[\"question\"]\n",
    "        gold_anchor = row[\"anchor\"]\n",
    "        gold_ctx = row[\"context_name\"] if row[\"has_context\"] == 1 else None\n",
    "        gold_has_ctx = int(row[\"has_context\"])\n",
    "\n",
    "        # Encode question\n",
    "        q_emb = encoder.encode([q], convert_to_tensor=True, device=DEVICE)\n",
    "        q_emb = q_emb.detach().clone()\n",
    "\n",
    "        q_anchor, q_ctx, q_hasctx = adapter(q_emb)\n",
    "\n",
    "        # ---------- ANCHOR CHECK ----------\n",
    "        sim_anchor = (q_anchor @ anchor_emb.T)[0]\n",
    "\n",
    "        for k in TOP_K:\n",
    "            if topk_check(anchors, sim_anchor, gold_anchor, k):\n",
    "                anchor_correct[k] += 1\n",
    "\n",
    "        # ---------- CONTEXT CHECK ----------\n",
    "        if gold_ctx and context_emb is not None:\n",
    "            sim_ctx = (q_ctx @ context_emb.T)[0]\n",
    "\n",
    "            for k in TOP_K:\n",
    "                if topk_check(contexts, sim_ctx, gold_ctx, k):\n",
    "                    context_correct[k] += 1\n",
    "\n",
    "        # ---------- HAS-CONTEXT ----------\n",
    "        hasctx_pred_prob = torch.sigmoid(q_hasctx).item()\n",
    "        hasctx_pred_bin = 1 if hasctx_pred_prob >= 0.5 else 0\n",
    "\n",
    "        hasctx_true.append(gold_has_ctx)\n",
    "        hasctx_pred.append(hasctx_pred_bin)\n",
    "\n",
    "    # =======================================================\n",
    "    # üìà Final Metrics\n",
    "    # =======================================================\n",
    "\n",
    "    print(\"\\n===============================\")\n",
    "    print(\"üìä FINAL EVALUATION RESULTS\")\n",
    "    print(\"===============================\")\n",
    "\n",
    "    print(\"\\nüéØ Anchor Accuracy:\")\n",
    "    for k in TOP_K:\n",
    "        print(f\"  Top-{k}: {anchor_correct[k] / total:.4f}\")\n",
    "\n",
    "    print(\"\\nüåç Context Accuracy (only samples with context):\")\n",
    "    for k in TOP_K:\n",
    "        print(f\"  Top-{k}: {context_correct[k] / ctx_total:.4f}\")\n",
    "\n",
    "    print(\"\\nüîÆ Has-Context F1 Score:\")\n",
    "    print(\"  F1:\", f\"{f1_score(hasctx_true, hasctx_pred):.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "307e0c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 409 anchors, 5 contexts.\n",
      "\n",
      "üîÑ Loading SentenceTransformer encoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from 'models_anchor/dual_expert_Advice_v3_0' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí¨ Ready! Nh·∫≠p c√¢u h·ªèi ƒë·ªÉ test (g√µ 'exit' ƒë·ªÉ tho√°t).\n",
      "\n",
      "üß™ Testing expert 'Advice' on question:\n",
      "‚û°Ô∏è  \"n√™n l√†m g√¨ khi b·ªã cao huy·∫øt √°p trong thai k·ª≥\"\n",
      "\n",
      "=== üîé RESULTS ===\n",
      "üéØ Predicted Anchor : Cao huy·∫øt √°p\n",
      "üåê Predicted Context: Thai k·ª≥\n",
      "üìà Has-Context prob : 0.9959\n",
      "\n",
      "Top 5 anchors:\n",
      "Cao huy·∫øt √°p                             ‚Üí 0.6793\n",
      "Cao huy·∫øt √°p m√£n t√≠nh                    ‚Üí 0.4766\n",
      "TƒÉng huy·∫øt √°p                            ‚Üí 0.4111\n",
      "Huy·∫øt √°p cao                             ‚Üí 0.3806\n",
      "TƒÉng huy·∫øt √°p m√£n t√≠nh                   ‚Üí 0.3542\n",
      "\n",
      "Top 5 contexts:\n",
      "Thai k·ª≥                                  ‚Üí 0.8825\n",
      "Cu·ªëi thai k·ª≥                             ‚Üí -0.0568\n",
      "Tr∆∞·ªõc khi thai k·ª≥                        ‚Üí -0.3305\n",
      "Qu√° tr√¨nh sinh n·ªü                        ‚Üí -0.3572\n",
      "Chuy·ªÉn d·∫°                                ‚Üí -0.3789\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 98\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müí¨ Ready! Nh·∫≠p c√¢u h·ªèi ƒë·ªÉ test (g√µ \u001b[39m\u001b[33m'\u001b[39m\u001b[33mexit\u001b[39m\u001b[33m'\u001b[39m\u001b[33m ƒë·ªÉ tho√°t).\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m     QUESTION = \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m‚ùì C√¢u h·ªèi: \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.strip()\n\u001b[32m     99\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m QUESTION.lower() == \u001b[33m\"\u001b[39m\u001b[33mexit\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    100\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müëã Bye!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py:1282\u001b[39m, in \u001b[36mKernel.raw_input\u001b[39m\u001b[34m(self, prompt)\u001b[39m\n\u001b[32m   1280\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1281\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[32m-> \u001b[39m\u001b[32m1282\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1283\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1284\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1285\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1286\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1287\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py:1325\u001b[39m, in \u001b[36mKernel._input_request\u001b[39m\u001b[34m(self, prompt, ident, parent, password)\u001b[39m\n\u001b[32m   1322\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1323\u001b[39m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[32m   1324\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mInterrupted by user\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1325\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1326\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1327\u001b[39m     \u001b[38;5;28mself\u001b[39m.log.warning(\u001b[33m\"\u001b[39m\u001b[33mInvalid Message:\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üß™ Test Dual Expert v3.0 (Anchor + Context + HasContext)\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# ‚öôÔ∏è CONFIGURATION\n",
    "# ------------------------------------------------------------\n",
    "EXPERT_NAME = \"Advice\"\n",
    "\n",
    "MODEL_PATH = f\"models_anchor/dual_expert_{EXPERT_NAME}_v3_0\"\n",
    "ADAPTER_PATH = os.path.join(MODEL_PATH, \"dual_adapter_v3.pt\")\n",
    "\n",
    "ANCHOR_FILE = \"anchors.csv\"\n",
    "CONTEXT_FILE = \"contexts.csv\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "TOP_K = 5\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# üß© Load Anchors + Contexts\n",
    "# ------------------------------------------------------------\n",
    "df_anchor = pd.read_csv(ANCHOR_FILE)\n",
    "ANCHORS = df_anchor[\"name\"].dropna().tolist()\n",
    "\n",
    "if os.path.exists(CONTEXT_FILE):\n",
    "    CONTEXTS = pd.read_csv(CONTEXT_FILE)[\"name\"].dropna().tolist()\n",
    "else:\n",
    "    CONTEXTS = []\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(ANCHORS)} anchors, {len(CONTEXTS)} contexts.\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# üß† Dual Expert Adapter v3.0\n",
    "# ------------------------------------------------------------\n",
    "class DualExpertAdapter(nn.Module):\n",
    "    def __init__(self, input_dim=768, hidden_dim=256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.anchor_adapter = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "        self.context_adapter = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "        self.has_context_head = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        a = F.normalize(self.anchor_adapter(x), p=2, dim=1)\n",
    "        c = F.normalize(self.context_adapter(x), p=2, dim=1)\n",
    "        h = self.has_context_head(x).squeeze(-1)\n",
    "        return a, c, h\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# üöÄ Load model + adapter\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nüîÑ Loading SentenceTransformer encoder...\")\n",
    "encoder = SentenceTransformer(MODEL_PATH).to(DEVICE)\n",
    "\n",
    "try:\n",
    "    EMB_DIM = encoder.get_sentence_embedding_dimension()\n",
    "except:\n",
    "    EMB_DIM = encoder.encode([\"hi\"], convert_to_tensor=True).shape[1]\n",
    "\n",
    "adapter = DualExpertAdapter(input_dim=EMB_DIM).to(DEVICE)\n",
    "adapter.load_state_dict(torch.load(ADAPTER_PATH, map_location=DEVICE))\n",
    "adapter.eval()\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# üßÆ Helper ‚Äî Encode function (v3.0)\n",
    "# ------------------------------------------------------------\n",
    "def encode_embed(texts):\n",
    "    emb = encoder.encode(texts, convert_to_tensor=True, device=DEVICE)\n",
    "    emb = emb.detach().clone()  # FIX inference mode\n",
    "    return adapter(emb)          # anchor_emb, context_emb, hasctx\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# üß™ INTERACTIVE TEST MODE\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nüí¨ Ready! Nh·∫≠p c√¢u h·ªèi ƒë·ªÉ test (g√µ 'exit' ƒë·ªÉ tho√°t).\")\n",
    "\n",
    "while True:\n",
    "    QUESTION = input(\"\\n‚ùì C√¢u h·ªèi: \").strip()\n",
    "    if QUESTION.lower() == \"exit\":\n",
    "        print(\"\\nüëã Bye!\")\n",
    "        break\n",
    "    if QUESTION == \"\":\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nüß™ Testing expert '{EXPERT_NAME}' on question:\\n‚û°Ô∏è  \\\"{QUESTION}\\\"\")\n",
    "\n",
    "    # Encode c√¢u h·ªèi\n",
    "    q_a, q_c, q_h = encode_embed([QUESTION])\n",
    "\n",
    "    # Encode anchors\n",
    "    a_embs, _, _ = encode_embed(ANCHORS)\n",
    "\n",
    "    # Encode contexts\n",
    "    if len(CONTEXTS) > 0:\n",
    "        _, c_embs, _ = encode_embed(CONTEXTS)\n",
    "    else:\n",
    "        c_embs = None\n",
    "\n",
    "    # ----- Compute similarity -----\n",
    "    sim_anchor = F.cosine_similarity(q_a, a_embs)\n",
    "    sim_context = F.cosine_similarity(q_c, c_embs) if c_embs is not None else None\n",
    "    has_ctx_prob = torch.sigmoid(q_h).item()\n",
    "\n",
    "    # ----- Best anchor -----\n",
    "    best_anchor = ANCHORS[torch.argmax(sim_anchor).item()]\n",
    "\n",
    "    # ----- Best context -----\n",
    "    if c_embs is not None and has_ctx_prob > 0.35:\n",
    "        best_context = CONTEXTS[torch.argmax(sim_context).item()]\n",
    "    else:\n",
    "        best_context = \"(no context)\"\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # üîé PRINT RESULTS\n",
    "    # --------------------------------------------------------\n",
    "    print(\"\\n=== üîé RESULTS ===\")\n",
    "    print(f\"üéØ Predicted Anchor : {best_anchor}\")\n",
    "    print(f\"üåê Predicted Context: {best_context}\")\n",
    "    print(f\"üìà Has-Context prob : {has_ctx_prob:.4f}\")\n",
    "\n",
    "    print(\"\\nTop 5 anchors:\")\n",
    "    for name, score in sorted(zip(ANCHORS, sim_anchor.tolist()), key=lambda x: x[1], reverse=True)[:TOP_K]:\n",
    "        print(f\"{name:<40} ‚Üí {score:.4f}\")\n",
    "\n",
    "    if c_embs is not None:\n",
    "        print(\"\\nTop 5 contexts:\")\n",
    "        for name, score in sorted(zip(CONTEXTS, sim_context.tolist()), key=lambda x: x[1], reverse=True)[:TOP_K]:\n",
    "            print(f\"{name:<40} ‚Üí {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1c7576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 408 anchors, 5 contexts.\n",
      "\n",
      "üîÑ Loading SentenceTransformer encoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from 'models_anchor/dual_expert_Advice_v3_0' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí¨ Ready! Nh·∫≠p c√¢u h·ªèi ƒë·ªÉ test (g√µ 'exit' ƒë·ªÉ tho√°t).\n",
      "\n",
      "üß™ Testing expert 'Advice' on question:\n",
      "‚û°Ô∏è  \"T√¥i b·ªã Ti·ªÅn ti·ªÉu ƒë∆∞·ªùng th√¨ n√™n l√†m g√¨\"\n",
      "\n",
      "=== üîé RESULTS ===\n",
      "üéØ Predicted Anchor : Ti·ªÅn ti·ªÉu ƒë∆∞·ªùng   (Disease)\n",
      "üåê Predicted Context: (no context)\n",
      "üìà Has-Context prob : 0.0004\n",
      "\n",
      "Top 5 anchors:\n",
      "Ti·ªÅn ti·ªÉu ƒë∆∞·ªùng                     (Disease     ) ‚Üí 0.7076\n",
      "Ti·ªÉu ƒë∆∞·ªùng                          (Disease     ) ‚Üí 0.4100\n",
      "Ti·ªÅn s·∫£n gi·∫≠t                       (Disease     ) ‚Üí 0.3636\n",
      "Ti·ªÅn s·∫£n gi·∫≠t                       (Topic       ) ‚Üí 0.3636\n",
      "Ch·∫©n ƒëo√°n ti·ªÅn s·∫£n                  (Topic       ) ‚Üí 0.2764\n",
      "\n",
      "Top 5 contexts:\n",
      "Thai k·ª≥                                  ‚Üí 0.3170\n",
      "Cu·ªëi thai k·ª≥                             ‚Üí 0.2872\n",
      "Chuy·ªÉn d·∫°                                ‚Üí 0.1193\n",
      "Qu√° tr√¨nh sinh n·ªü                        ‚Üí -0.2940\n",
      "Tr∆∞·ªõc khi thai k·ª≥                        ‚Üí -0.4054\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üß™ Test Dual Expert v3.0 (with Anchor Layer Output)\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# ‚öôÔ∏è CONFIGURATION\n",
    "# ------------------------------------------------------------\n",
    "EXPERT_NAME = \"Advice\"\n",
    "\n",
    "MODEL_PATH = f\"models_anchor/dual_expert_{EXPERT_NAME}_v3_0\"\n",
    "ADAPTER_PATH = os.path.join(MODEL_PATH, \"dual_adapter_v3.pt\")\n",
    "\n",
    "ANCHOR_FILE = \"anchors.csv\"\n",
    "CONTEXT_FILE = \"contexts.csv\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "TOP_K = 5\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# üß© Load Anchors + Layers + Contexts\n",
    "# ------------------------------------------------------------\n",
    "df_anchor = pd.read_csv(ANCHOR_FILE)\n",
    "\n",
    "ANCHORS = df_anchor[\"name\"].tolist()\n",
    "ANCHOR_LAYERS = df_anchor[\"layer\"].tolist()   # ‚¨ÖÔ∏è LAYER HERE\n",
    "\n",
    "if os.path.exists(CONTEXT_FILE):\n",
    "    CONTEXTS = pd.read_csv(CONTEXT_FILE)[\"name\"].dropna().tolist()\n",
    "else:\n",
    "    CONTEXTS = []\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(ANCHORS)} anchors, {len(CONTEXTS)} contexts.\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# üß† Dual Expert Adapter v3.0\n",
    "# ------------------------------------------------------------\n",
    "class DualExpertAdapter(nn.Module):\n",
    "    def __init__(self, input_dim=768, hidden_dim=256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.anchor_adapter = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "        self.context_adapter = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "        self.has_context_head = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        a = F.normalize(self.anchor_adapter(x), p=2, dim=1)\n",
    "        c = F.normalize(self.context_adapter(x), p=2, dim=1)\n",
    "        h = self.has_context_head(x).squeeze(-1)\n",
    "        return a, c, h\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# üöÄ Load model + adapter\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nüîÑ Loading SentenceTransformer encoder...\")\n",
    "encoder = SentenceTransformer(MODEL_PATH).to(DEVICE)\n",
    "\n",
    "try:\n",
    "    EMB_DIM = encoder.get_sentence_embedding_dimension()\n",
    "except:\n",
    "    EMB_DIM = encoder.encode([\"hi\"], convert_to_tensor=True).shape[1]\n",
    "\n",
    "adapter = DualExpertAdapter(input_dim=EMB_DIM).to(DEVICE)\n",
    "adapter.load_state_dict(torch.load(ADAPTER_PATH, map_location=DEVICE))\n",
    "adapter.eval()\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# üßÆ Helper ‚Äî Encode function (v3.0)\n",
    "# ------------------------------------------------------------\n",
    "def encode_embed(texts):\n",
    "    emb = encoder.encode(texts, convert_to_tensor=True, device=DEVICE)\n",
    "    emb = emb.detach().clone()  # FIX inference mode\n",
    "    return adapter(emb)          # anchor_emb, context_emb, hasctx\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# üß™ INTERACTIVE TEST MODE\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nüí¨ Ready! Nh·∫≠p c√¢u h·ªèi ƒë·ªÉ test (g√µ 'exit' ƒë·ªÉ tho√°t).\")\n",
    "\n",
    "while True:\n",
    "    QUESTION = input(\"\\n‚ùì C√¢u h·ªèi: \").strip()\n",
    "    if QUESTION.lower() == \"exit\":\n",
    "        print(\"\\nüëã Bye!\")\n",
    "        break\n",
    "    if QUESTION == \"\":\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nüß™ Testing expert '{EXPERT_NAME}' on question:\\n‚û°Ô∏è  \\\"{QUESTION}\\\"\")\n",
    "\n",
    "    # Encode c√¢u h·ªèi\n",
    "    q_a, q_c, q_h = encode_embed([QUESTION])\n",
    "\n",
    "    # Encode anchors & contexts\n",
    "    a_embs, _, _ = encode_embed(ANCHORS)\n",
    "    if len(CONTEXTS) > 0:\n",
    "        _, c_embs, _ = encode_embed(CONTEXTS)\n",
    "    else:\n",
    "        c_embs = None\n",
    "\n",
    "    # ----- Similarity -----\n",
    "    sim_anchor = F.cosine_similarity(q_a, a_embs)\n",
    "    sim_context = F.cosine_similarity(q_c, c_embs) if c_embs is not None else None\n",
    "    has_ctx_prob = torch.sigmoid(q_h).item()\n",
    "\n",
    "    # ----- Best anchor -----\n",
    "    best_idx = torch.argmax(sim_anchor).item()\n",
    "    best_anchor = ANCHORS[best_idx]\n",
    "    best_layer = ANCHOR_LAYERS[best_idx]  # ‚¨ÖÔ∏è PRINT LAYER\n",
    "\n",
    "    # ----- Best context -----\n",
    "    if c_embs is not None and has_ctx_prob > 0.35:\n",
    "        best_context = CONTEXTS[torch.argmax(sim_context).item()]\n",
    "    else:\n",
    "        best_context = \"(no context)\"\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # üîé PRINT RESULTS (now with layer!)\n",
    "    # --------------------------------------------------------\n",
    "    print(\"\\n=== üîé RESULTS ===\")\n",
    "    print(f\"üéØ Predicted Anchor : {best_anchor}   ({best_layer})\")\n",
    "    print(f\"üåê Predicted Context: {best_context}\")\n",
    "    print(f\"üìà Has-Context prob : {has_ctx_prob:.4f}\")\n",
    "\n",
    "    print(\"\\nTop 5 anchors:\")\n",
    "    top5 = sorted(\n",
    "        list(zip(ANCHORS, ANCHOR_LAYERS, sim_anchor.tolist())),\n",
    "        key=lambda x: x[2], reverse=True\n",
    "    )[:TOP_K]\n",
    "\n",
    "    for name, layer, score in top5:\n",
    "        print(f\"{name:<35} ({layer:<12}) ‚Üí {score:.4f}\")\n",
    "\n",
    "    if c_embs is not None:\n",
    "        print(\"\\nTop 5 contexts:\")\n",
    "        top5_ctx = sorted(\n",
    "            list(zip(CONTEXTS, sim_context.tolist())),\n",
    "            key=lambda x: x[1], reverse=True\n",
    "        )[:TOP_K]\n",
    "\n",
    "        for name, score in top5_ctx:\n",
    "            print(f\"{name:<40} ‚Üí {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed2b806",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
